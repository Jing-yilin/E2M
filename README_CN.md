# E2M (Everything to Markdown)

<p align="center">
    <a href="https://github.com/Jing-yilin/E2M">
        <img src="./assets/logo.png" alt="E2M Logo" style="width: 200px;">
    </a>
</p>

<p align="center">
    <a href="https://github.com/Jing-yilin/E2M">
        <img src="https://img.shields.io/badge/E2M-repo-blue" alt="E2M Repo">
    </a>
    <a href="https://github.com/Jing-yilin/E2M/tags/v1.1.2">
        <img src="https://img.shields.io/badge/version-v1.1.2-blue" alt="E2M Version">
    </a>
    <a href="https://hub.docker.com/r/jingyilin/e2m/tags">
        <img src="https://img.shields.io/badge/docker-repo-blue" alt="Docker Repo">
    </a>
    <a href="https://github.com/Jing-yilin/E2M/blob/main/LICENSE">
        <img src="https://img.shields.io/badge/license-Apache%202.0-blue" alt="E2M License">
    </a>
    <a href="https://www.python.org/downloads/">
        <img src="https://img.shields.io/badge/python-3.10%20%7C%203.11-blue" alt="Python Version">
    </a>
</p>

<div align="center">
  <a href="./README.md"><img alt="en" src="https://img.shields.io/badge/è‹±è¯­-d9d9d9"></a>
  <a href="./README_CN.md"><img alt="zh" src="https://img.shields.io/badge/ç®€ä½“ä¸­æ–‡-d9d9d9"></a>
</div>

- [E2M (Everything to Markdown)](#e2m-everything-to-markdown)
  - [ğŸŒŸ ä»‹ç»](#-ä»‹ç»)
    - [ğŸŒ ç½‘é¡µ](#-ç½‘é¡µ)
      - [ğŸ“ƒ è½¬æ¢ä¸º Markdown](#-è½¬æ¢ä¸º-markdown)
      - [ğŸ“ƒ è½¬æ¢ä¸º Json](#-è½¬æ¢ä¸º-json)
    - [ğŸ“¸ æ¼”ç¤º](#-æ¼”ç¤º)
    - [ğŸ“‚ æ”¯æŒçš„æ–‡ä»¶ç±»å‹](#-æ”¯æŒçš„æ–‡ä»¶ç±»å‹)
    - [ğŸ—³ï¸ å¾…å®Œæˆ](#ï¸-å¾…å®Œæˆ)
  - [ğŸš€ å¿«é€Ÿå¼€å§‹](#-å¿«é€Ÿå¼€å§‹)
    - [ğŸ“¦ å¿«é€Ÿå¯åŠ¨ï¼ˆè¿œç¨‹ dockerï¼‰](#-å¿«é€Ÿå¯åŠ¨è¿œç¨‹-docker)
    - [ğŸ¬ è¿è¡Œæœ¬åœ° Docker](#-è¿è¡Œæœ¬åœ°-docker)
    - [ğŸ¬ è¿è¡Œæœ¬åœ° Docker-GPU ç‰ˆæœ¬](#-è¿è¡Œæœ¬åœ°-docker-gpu-ç‰ˆæœ¬)
      - [ğŸ§Ubuntu](#ubuntu)
      - [ğŸ–¥ï¸Windows](#ï¸windows)
    - [âš™ï¸ ä»æºç å¼€å§‹](#ï¸-ä»æºç å¼€å§‹)
      - [ğŸ§Ubuntu](#ubuntu-1)
      - [ğŸMac](#mac)
      - [ğŸ–¥ï¸Windows](#ï¸windows-1)
    - [ğŸ”§ è®¾ç½®å¼€å‘ç¯å¢ƒ](#-è®¾ç½®å¼€å‘ç¯å¢ƒ)
    - [ğŸ­ è®¾ç½®ç”Ÿäº§ç¯å¢ƒ](#-è®¾ç½®ç”Ÿäº§ç¯å¢ƒ)
    - [ğŸ“– å¦‚ä½•ä½¿ç”¨](#-å¦‚ä½•ä½¿ç”¨)
    - [ğŸ”– è¯­è¨€æ”¯æŒ](#-è¯­è¨€æ”¯æŒ)
  - [ğŸ¤ å¦‚ä½•è´¡çŒ®](#-å¦‚ä½•è´¡çŒ®)
    - [ğŸŒ¿ åˆ›å»ºæ–°åˆ†æ”¯](#-åˆ›å»ºæ–°åˆ†æ”¯)
    - [ğŸ“PEP8 é£æ ¼](#pep8-é£æ ¼)
    - [ğŸ”„ æ¨é€åˆ°è¿œç¨‹ä»“åº“](#-æ¨é€åˆ°è¿œç¨‹ä»“åº“)
    - [ğŸ³ æ¨é€åˆ° Docker](#-æ¨é€åˆ°-docker)
    - [ğŸ”€ æ‹‰å–è¯·æ±‚](#-æ‹‰å–è¯·æ±‚)
  - [ğŸŒŸ è´¡çŒ®è€…](#-è´¡çŒ®è€…)
    - [ğŸ‘¥ è´¡çŒ®è€…åå•](#-è´¡çŒ®è€…åå•)
  - [ğŸ“± ç¤¾åŒº](#-ç¤¾åŒº)
    - [å¾®ä¿¡](#å¾®ä¿¡)
    - [Discord](#discord)

## ğŸŒŸ ä»‹ç»

âœ¨ å½“å‰ç‰ˆæœ¬: `v1.1.2`

ğŸ¦„E2M æ˜¯ä¸€ä¸ªèƒ½å¤Ÿå°†æ‰€æœ‰æ–‡ä»¶è½¬æ¢ä¸º Markdown æˆ– JSONï¼ˆä¸¤è€…éƒ½æ˜¯ LLM å‹å¥½æ ¼å¼ï¼‰çš„ API å·¥å…·ã€‚

ğŸ”¥ æœ€å¥½åœ¨ç¯å¢ƒä¸­è®¾ç½® `USE_LLM=True` å¹¶ä½¿ç”¨ LLM API ä»¥è·å¾—æœ€ä½³ç»“æœã€‚

> ä¸ºä»€ä¹ˆæˆ‘åˆ›å»ºè¿™ä¸ª APIï¼Ÿå› ä¸ºæˆ‘åšä¿¡åœ¨è¿™ä¸ª AI æ—¶ä»£ï¼Œæ•°æ®æ˜¯æœ€é‡è¦çš„ä¸œè¥¿ï¼Œä½†è®¸å¤šèµ„æºå¹¶ä¸æ˜¯ä»¥æ­£ç¡®çš„æ ¼å¼å­˜åœ¨çš„ã€‚**å®ƒä»¬åªæ˜¯ä¿¡æ¯ï¼Œè€Œä¸æ˜¯æ•°æ®ã€‚** æ‰€ä»¥æˆ‘æƒ³åˆ›å»ºä¸€ä¸ªå·¥å…·ï¼Œå°†ä¸€åˆ‡è½¬æ¢ä¸º Markdown æˆ– JSONï¼Œè¿™åœ¨ AI é¢†åŸŸæ˜¯æœ€å¸¸è§çš„æ ¼å¼ã€‚æˆ‘å¸Œæœ› E2M èƒ½è¢«ç”¨åœ¨ä»»ä½•éœ€è¦æ ¼å¼è½¬æ¢çš„ AI åº”ç”¨ä¸Šï¼Œæ¯”å¦‚ AI çŸ¥è¯†åº“ã€AI æ•°æ®é›†ç­‰ï¼Œé‚£ä¹ˆå¼€å‘è€…å°±å¯ä»¥ä¸“æ³¨äº AI åº”ç”¨çš„æ ¸å¿ƒåŠŸèƒ½ï¼Œè€Œä¸æ˜¯æ•°æ®æ ¼å¼è½¬æ¢ã€‚

### ğŸŒ ç½‘é¡µ

![Input Form](assets/web_01.png)

#### ğŸ“ƒ è½¬æ¢ä¸º Markdown

![Conversion Result](assets/web_02.png)

![API Response](assets/web_03.png)

<details>
```markdown
{
"error": null,
"json_data": null,
"md_data": {
"content": "# Attention Is All You Need\n\n**Authors:**\n\n- Ashish Vaswani, Google Brain, avaswani@google.com\n- Noam Shazeer, Google Brain, noam@google.com\n- Niki Parmar, Google Research, nikip@google.com\n- Jakob Uszkoreit, Google Research, usz@google.com\n- Llion Jones, Google Research, llion@google.com\n- Aidan N. Gomez, University of Toronto, aidan@cs.toronto.edu\n- Åukasz Kaiser, Google Brain, lukaszkaiser@google.com\n- Illia Polosukhin, illia.polosukhin@gmail.com\n\n**Abstract:**\nThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.\n\n## Introduction\n\nRecurrent neural networks, long short-term memory and gated recurrent neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures.\n\n**Contributions:**\n\n- _Equal contribution. Listing order is random._\n- Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea.\n- Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work.\n- Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail.\n- Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor.\n- Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations.\n- Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.\n\n**Affiliations:**\n\n- _â€ Work performed while at Google Brain._\n- _â€¡Work performed while at Google Research._\n\n*31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.*",
"elements": [
{
"children": [
{
"raw": "Attention Is All You Need",
"type": "text"
}
],
"text": "# Attention Is All You Need",
"type": "header1"
},
{
"children": [
{
"children": [
{
"raw": "Authors:",
"type": "text"
}
],
"type": "strong"
}
],
"text": "**Authors:**",
"type": "paragraph"
},
{
"children": [
{
"children": [
{
"children": [
{
"raw": "Ashish Vaswani, Google Brain, avaswani@google.com",
"type": "text"
}
],
"type": "block_text"
}
],
"type": "list_item"
},
{
"children": [
{
"children": [
{
"raw": "Noam Shazeer, Google Brain, noam@google.com",
"type": "text"
}
],
"type": "block_text"
}
],
"type": "list_item"
},
{
"children": [
{
"children": [
{
"raw": "Niki Parmar, Google Research, nikip@google.com",
"type": "text"
}
],
"type": "block_text"
}
],
"type": "list_item"
},
{
"children": [
{
"children": [
{
"raw": "Jakob Uszkoreit, Google Research, usz@google.com",
"type": "text"
}
],
"type": "block_text"
}
],
"type": "list_item"
},
{
"children": [
{
"children": [
{
"raw": "Llion Jones, Google Research, llion@google.com",
"type": "text"
}
],
"type": "block_text"
}
],
"type": "list_item"
},
{
"children": [
{
"children": [
{
"raw": "Aidan N. Gomez, University of Toronto, aidan@cs.toronto.edu",
"type": "text"
}
],
"type": "block_text"
}
],
"type": "list_item"
},
{
"children": [
{
"children": [
{
"raw": "Åukasz Kaiser, Google Brain, lukaszkaiser@google.com",
"type": "text"
}
],
"type": "block_text"
}
],
"type": "list_item"
},
{
"children": [
{
"children": [
{
"raw": "Illia Polosukhin, illia.polosukhin@gmail.com",
"type": "text"
}
],
"type": "block_text"
}
],
"type": "list_item"
}
],
"text": "- Ashish Vaswani, Google Brain, avaswani@google.com\n- Noam Shazeer, Google Brain, noam@google.com\n- Niki Parmar, Google Research, nikip@google.com\n- Jakob Uszkoreit, Google Research, usz@google.com\n- Llion Jones, Google Research, llion@google.com\n- Aidan N. Gomez, University of Toronto, aidan@cs.toronto.edu\n- Åukasz Kaiser, Google Brain, lukaszkaiser@google.com\n- Illia Polosukhin, illia.polosukhin@gmail.com",
"type": "list"
},
{
"children": [
{
"children": [
{
"raw": "Abstract:",
"type": "text"
}
],
"type": "strong"
},
{
"type": "softbreak"
},
{
"raw": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.",
"type": "text"
}
],
"text": "**Abstract:**\nThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.",
"type": "paragraph"
},
{
"children": [
{
"raw": "Introduction",
"type": "text"
}
],
"text": "## Introduction",
"type": "header2"
},
{
"children": [
{
"raw": "Recurrent neural networks, long short-term memory and gated recurrent neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures.",
"type": "text"
}
],
"text": "Recurrent neural networks, long short-term memory and gated recurrent neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures.",
"type": "paragraph"
},
{
"children": [
{
"children": [
{
"raw": "Contributions:",
"type": "text"
}
],
"type": "strong"
}
],
"text": "**Contributions:**",
"type": "paragraph"
},
{
"children": [
{
"children": [
{
"children": [
{
"children": [
{
"raw": "Equal contribution. Listing order is random.",
"type": "text"
}
],
"type": "emphasis"
}
],
"type": "block_text"
}
],
"type": "list_item"
},
{
"children": [
{
"children": [
{
"raw": "Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea.",
"type": "text"
}
],
"type": "block_text"
}
],
"type": "list_item"
},
{
"children": [
{
"children": [
{
"raw": "Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work.",
"type": "text"
}
],
"type": "block_text"
}
],
"type": "list_item"
},
{
"children": [
{
"children": [
{
"raw": "Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail.",
"type": "text"
}
],
"type": "block_text"
}
],
"type": "list_item"
},
{
"children": [
{
"children": [
{
"raw": "Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor.",
"type": "text"
}
],
"type": "block_text"
}
],
"type": "list_item"
},
{
"children": [
{
"children": [
{
"raw": "Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations.",
"type": "text"
}
],
"type": "block_text"
}
],
"type": "list_item"
},
{
"children": [
{
"children": [
{
"raw": "Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.",
"type": "text"
}
],
"type": "block_text"
}
],
"type": "list_item"
}
],
"text": "- _Equal contribution. Listing order is random._\n- Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea.\n- Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work.\n- Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail.\n- Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor.\n- Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations.\n- Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.",
"type": "list"
},
{
"children": [
{
"children": [
{
"raw": "Affiliations:",
"type": "text"
}
],
"type": "strong"
}
],
"text": "**Affiliations:**",
"type": "paragraph"
},
{
"children": [
{
"children": [
{
"children": [
{
"children": [
{
"raw": "â€ Work performed while at Google Brain.",
"type": "text"
}
],
"type": "emphasis"
}
],
"type": "block_text"
}
],
"type": "list_item"
},
{
"children": [
{
"children": [
{
"children": [
{
"raw": "â€¡Work performed while at Google Research.",
"type": "text"
}
],
"type": "emphasis"
}
],
"type": "block_text"
}
],
"type": "list_item"
}
],
"text": "- _â€ Work performed while at Google Brain._\n- _â€¡Work performed while at Google Research._",
"type": "list"
},
{
"children": [
{
"children": [
{
"raw": "31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.",
"type": "text"
}
],
"type": "emphasis"
}
],
"text": "_31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA._",
"type": "paragraph"
}
],
"toc": [
{
"text": "Attention Is All You Need",
"type": "header1"
},
{
"text": "Introduction",
"type": "header2"
}
]
},
"metadata": {
"file_info": {
"file_hash": "7a6bb1fcdceec29ff330d3af68fbe5d8",
"file_name": "Attention is All You Need Paper.pdf",
"file_path": "./temp/Attention is All You Need Paper.pdf",
"file_size": 569417,
"file_type": "pdf"
},
"llm_info": {
"completion_tokens": 637,
"messages": null,
"model": "gpt-3.5-turbo",
"prompt_tokens": 826,
"successful_requests": 1,
"total_cost": 0.0025129999999999996,
"total_tokens": 1463
},
"request_data": {
"enforced_json_format": "",
"extract_images": false,
"file_hash": "7a6bb1fcdceec29ff330d3af68fbe5d8",
"first_page": 1,
"langs": [
"en",
"zh"
],
"last_page": 1,
"model": "gpt-3.5-turbo",
"parse_mode": "auto",
"return_type": "md",
"save_to_cache": false,
"use_cache": false,
"use_llm": true
}
},
"raw": "Attention Is All You Need\n\nAshish Vaswaniâˆ— Google Brain avaswani@google.com\n\nNoam Shazeerâˆ— Google Brain noam@google.com\n\nNiki Parmarâˆ— Google Research nikip@google.com\n\nJakob Uszkoreitâˆ— Google Research usz@google.com\n\nLlion Jonesâˆ— Google Research llion@google.com\n\nAidan N. Gomezâˆ— â€  University of Toronto aidan@cs.toronto.edu\n\nÅukasz Kaiserâˆ— Google Brain lukaszkaiser@google.com\n\nIllia Polosukhinâˆ— â€¡ illia.polosukhin@gmail.com\n\nAbstract\n\nThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiï¬cantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.\n\n1\n\nIntroduction\n\nRecurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks in particular, have been ï¬rmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [29, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [31, 21, 13].\n\nâˆ—Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the ï¬rst Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efï¬cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.\n\nâ€ Work performed while at Google Brain. â€¡Work performed while at Google Research.\n\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.",
"status": "success"
}
```
</details>

#### ğŸ“ƒ è½¬æ¢ä¸º Json

![Conversion Result](assets/web_04.png)

<details>
```json
{
    "error": null,
    "json_data": {
        "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.",
        "authors": [
            {
                "affiliation": "Google Brain",
                "email": "avaswani@google.com",
                "name": "Ashish Vaswani"
            },
            {
                "affiliation": "Google Brain",
                "email": "noam@google.com",
                "name": "Noam Shazeer"
            },
            {
                "affiliation": "Google Research",
                "email": "nikip@google.com",
                "name": "Niki Parmar"
            },
            {
                "affiliation": "Google Research",
                "email": "usz@google.com",
                "name": "Jakob Uszkoreit"
            },
            {
                "affiliation": "Google Research",
                "email": "llion@google.com",
                "name": "Llion Jones"
            },
            {
                "affiliation": "University of Toronto",
                "email": "aidan@cs.toronto.edu",
                "name": "Aidan N. Gomez"
            },
            {
                "affiliation": "Google Brain",
                "email": "lukaszkaiser@google.com",
                "name": "Åukasz Kaiser"
            },
            {
                "email": "illia.polosukhin@gmail.com",
                "name": "Illia Polosukhin"
            }
        ],
        "conference": {
            "location": "Long Beach, CA, USA",
            "name": "31st Conference on Neural Information Processing Systems (NIPS 2017)"
        },
        "introduction": "Recurrent neural networks, long short-term memory and gated recurrent neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.",
        "title": "Attention Is All You Need"
    },
    "md_data": null,
    "metadata": {
        "file_info": {
            "file_hash": "7a6bb1fcdceec29ff330d3af68fbe5d8",
            "file_name": "Attention is All You Need Paper.pdf",
            "file_path": "./temp/Attention is All You Need Paper.pdf",
            "file_size": 569417,
            "file_type": "pdf"
        },
        "llm_info": {
            "completion_tokens": 761,
            "messages": null,
            "model": "gpt-3.5-turbo",
            "prompt_tokens": 843,
            "successful_requests": 1,
            "total_cost": 0.0027865,
            "total_tokens": 1604
        },
        "request_data": {
            "enforced_json_format": "",
            "extract_images": false,
            "file_hash": "7a6bb1fcdceec29ff330d3af68fbe5d8",
            "first_page": 1,
            "langs": ["en", "zh"],
            "last_page": 1,
            "model": "gpt-3.5-turbo",
            "parse_mode": "auto",
            "return_type": "json",
            "save_to_cache": false,
            "use_cache": false,
            "use_llm": true
        }
    },
    "raw": "Attention Is All You Need\n\nAshish Vaswaniâˆ— Google Brain avaswani@google.com\n\nNoam Shazeerâˆ— Google Brain noam@google.com\n\nNiki Parmarâˆ— Google Research nikip@google.com\n\nJakob Uszkoreitâˆ— Google Research usz@google.com\n\nLlion Jonesâˆ— Google Research llion@google.com\n\nAidan N. Gomezâˆ— â€  University of Toronto aidan@cs.toronto.edu\n\nÅukasz Kaiserâˆ— Google Brain lukaszkaiser@google.com\n\nIllia Polosukhinâˆ— â€¡ illia.polosukhin@gmail.com\n\nAbstract\n\nThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiï¬cantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.\n\n1\n\nIntroduction\n\nRecurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks in particular, have been ï¬rmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [29, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [31, 21, 13].\n\nâˆ—Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the ï¬rst Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efï¬cient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research.\n\nâ€ Work performed while at Google Brain. â€¡Work performed while at Google Research.\n\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.",
    "status": "success"
}
```
</details>

### ğŸ“¸ æ¼”ç¤º

![image-20240528122849203](assets/demo_01.png)

![image-20240528123852545](assets/demo_02.png)

![image-20240528124726338](assets/demo_03.png)

### ğŸ“‚ æ”¯æŒçš„æ–‡ä»¶ç±»å‹

<table style="width: 100%;">
  <tr>
    <th align="center">æ”¯æŒçš„ç±»å‹</th>
    <th align="center">æ–‡æ¡£</th>
    <th align="center">å›¾ç‰‡</th>
    <th align="center">æ•°æ®</th>
    <th align="center">éŸ³é¢‘</th>
    <th align="center">è§†é¢‘</th>
  </tr>
  <tr>
    <td align="center">å®Œæˆ</td>
    <td align="center">doc, docx, ppt, pptx, pdf, html, htm</td>
    <td align="center"></td>
    <td align="center"></td>
    <td align="center"></td>
    <td align="center"></td>
  </tr>
  <tr>
    <td align="center">å¾…å®Œæˆ</td>
    <td align="center"></td>
    <td align="center">jpg, jpeg, png, gif, svg</td>
    <td align="center">csv, xlsx, xls</td>
    <td align="center">mp3, wav, flac</td>
    <td align="center">mp4, avi, mkv</td>
  </tr>
</table>

### ğŸ—³ï¸ å¾…å®Œæˆ

-   [x] è§£ææ¨¡å¼ï¼š`auto`ï¼Œ`ocr-low(tesseract)`ï¼Œ`ocr-high(Surya)`ï¼Œ`fast`
-   [x] æ›´æ–° API ç»“æ„
-   [ ] æ”¯æŒé•¿æ–‡æ¡£è§£æ
-   [ ] æ·»åŠ ä¸€ä¸ªæ–°è¡¨æ¥å­˜å‚¨åŸå§‹æ•°æ®
-   [ ] åœ¨ API å’Œå‰ç«¯æ·»åŠ æµæ¨¡å¼
-   [ ] åœ¨ API ä¸­æ·»åŠ å¼‚æ­¥åŠŸèƒ½
-   [ ] ä¸º E2M API å¼€å‘ä¸€ä¸ª SDK
-   [ ] æ·»åŠ æ›´å¤šçš„ LLM API
-   [ ] å¼€æ”¾ä¸€ä¸ªåœ¨çº¿æ¼”ç¤º

## ğŸš€ å¿«é€Ÿå¼€å§‹

åœ¨å¯åŠ¨å‰ï¼Œæ‚¨éœ€è¦æŸ¥çœ‹å¹³å°æ¶æ„ä¿¡æ¯:

```bash
$ arch
```

1. å¦‚æœæ˜¯ `x86_64`ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨:
    - `docker-compose.amd64.yml`
    - `docker-compose.gpu.amd64.yml`
2. å¦‚æœæ˜¯ `arm64`ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨:
    - `docker-compose.arm64.yml`
    - `docker-compose.gpu.arm64.yml`

### ğŸ“¦ å¿«é€Ÿå¯åŠ¨ï¼ˆè¿œç¨‹ dockerï¼‰

> æ‚¨éœ€è¦æå‰å®‰è£… `docker` å’Œ `docker-compose`ã€‚

```bash
git clone https://github.com/Jing-yilin/E2M
cd E2M/docker
# ç¼–è¾‘ docker-compose.yml æ–‡ä»¶ï¼Œå°† `USE_LLM` è®¾ç½®ä¸º `True`,å¹¶æ·»åŠ æ‚¨çš„APIå¯†é’¥
# æ‚¨éœ€è¦é€‰æ‹©å¥½å¯¹åº”çš„docker-composeæ–‡ä»¶è¿›è¡Œéƒ¨ç½²
docker-compose -f docker-compose.amd64.yml up --build -d
# æŸ¥çœ‹æ—¥å¿—
docker-compose -f docker-compose.amd64.yml logs -f
# åˆ é™¤å®¹å™¨
docker-compose -f docker-compose.amd64.yml down
```

å¦‚æœæ‚¨æƒ³ä½¿ç”¨ GPU ç‰ˆæœ¬ï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤(ç›®å‰æš‚ä¸æ”¯æŒ mps)ï¼š

```bash
docker-compose -f docker-compose.gpu.yml up --build -d
```

-   ğŸš€ ç½‘é¡µï¼š[http://127.0.0.1:3000](http://127.0.0.1:3000)
-   ğŸš€APIï¼š[http://127.0.0.1:8765/api/v1/](http://127.0.0.1:8765/api/v1/)
-   ğŸš€API æ–‡æ¡£ï¼š[http://127.0.0.1:8765/swagger/](http://127.0.0.1:8765/swagger/)

### ğŸ¬ è¿è¡Œæœ¬åœ° Docker

> æ‚¨éœ€è¦æå‰å®‰è£… `docker` å’Œ `docker-compose`ã€‚

```bash
git clone https://github.com/Jing-yilin/E2M
cd E2M
# ç¼–è¾‘ docker-compose.yml æ–‡ä»¶ï¼Œå°† `USE_LLM` è®¾ç½®ä¸º `True`,å¹¶æ·»åŠ æ‚¨çš„APIå¯†é’¥
# éƒ¨ç½²åº”ç”¨åˆ° docker
docker-compose up --build -d
# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f
# åˆ é™¤å®¹å™¨
docker-compose down
```

-   ğŸš€ ç½‘é¡µï¼š[http://127.0.0.1:3000](http://127.0.0.1:3000)
-   ğŸš€APIï¼š[http://127.0.0.1:8765/api/v1/](http://127.0.0.1:8765/api/v1/)
-   ğŸš€API æ–‡æ¡£ï¼š[http://127.0.0.1:8765/swagger/](http://127.0.0.1:8765/swagger/)

### ğŸ¬ è¿è¡Œæœ¬åœ° Docker-GPU ç‰ˆæœ¬

#### ğŸ§Ubuntu

è¦åˆ©ç”¨æœ¬åœ° GPUï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š

1. å®‰è£… NVIDIA é©±åŠ¨ç¨‹åºï¼šç¡®ä¿åœ¨ä¸»æœºä¸Šå®‰è£…äº† NVIDIA é©±åŠ¨ç¨‹åºã€‚

2. å®‰è£… NVIDIA å®¹å™¨å·¥å…·åŒ…ï¼š

```bash
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list
sudo apt-get update
sudo apt-get install -y nvidia-docker2
sudo systemctl restart docker
```

> å¦‚æœé‡åˆ°ä»»ä½•é—®é¢˜ï¼Œå¯èƒ½éœ€è¦æ›´æ–° docker ç‰ˆæœ¬ã€‚

1. è¿è¡Œæ”¯æŒ GPU çš„ Docker å®¹å™¨ï¼š

```bash
# è¿è¡Œå®¹å™¨
docker-compose -f docker-compose.gpu.yml up --build -d
# ç¼–è¾‘ docker-compose.yml æ–‡ä»¶ï¼Œå°† `USE_LLM` è®¾ç½®ä¸º `True`,å¹¶æ·»åŠ æ‚¨çš„APIå¯†é’¥
# æŸ¥çœ‹æ—¥å¿—
docker-compose -f docker-compose.gpu.yml logs -f
# åˆ é™¤å®¹å™¨
docker-compose -f docker-compose.gpu.yml down
```

-   ğŸš€ ç½‘é¡µï¼š[http://127.0.0.1:3000](http://127.0.0.1:3000)
-   ğŸš€APIï¼š[http://127.0.0.1:8765/api/v1/](http://127.0.0.1:8765/api/v1/)
-   ğŸš€API æ–‡æ¡£ï¼š[http://127.0.0.1:8765/swagger/](http://127.0.0.1:8765/swagger/)

#### ğŸ–¥ï¸Windows

å¦‚æœä½ ä½¿ç”¨ Windowsï¼Œä½ å¯ä»¥ä½¿ç”¨ Docker Desktop æ¥æ”¯æŒ GPUï¼š

> å®‰è£… gpu ç‰ˆ docker è¯·å‚è€ƒ: [https://docs.docker.com/desktop/gpu/](https://docs.docker.com/desktop/gpu/)

ç„¶åï¼Œä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯åŠ¨å®¹å™¨ï¼š

```bash
docker-compose -f docker-compose.gpu.yml up --build -d
# check the logs with
docker-compose -f docker-compose.gpu.yml logs -f
# remove the container with
docker-compose -f docker-compose.gpu.yml down
```

### âš™ï¸ ä»æºç å¼€å§‹

å®‰è£…ï¼š

```bash
git clone https://github.com/Jing-yilin/E2M
cd E2M/app
conda create -n e2m python=3.10 -y
conda activate e2m
python -m pip install -r requirements-dev.txt
```

é¦–å…ˆï¼Œä½ åº”è¯¥å®‰è£… `postgresql@15.0` å’Œ `libreoffice`ï¼š

#### ğŸ§Ubuntu

1. å®‰è£… PostgreSQL 15 å’Œ LibreOfficeï¼š

    > å‚è€ƒï¼š[å¦‚ä½•åœ¨ Ubuntu ä¸Šå®‰è£… PostgreSQL](https://www.linuxtechi.com/how-to-install-postgresql-on-ubuntu/)

    ```sh
    sudo sh -c 'echo "deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main" > /etc/apt/sources.list.d/pgdg.list'
    wget -qO- https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo tee /etc/apt/trusted.gpg.d/pgdg.asc &>/dev/null
    sudo apt update
    sudo apt install postgresql-15 postgresql-client-15 -y
    sudo apt install libreoffice -y
    ```

2. å¯åŠ¨ PostgreSQLï¼š
    ```sh
    sudo systemctl status postgresql
    ```

#### ğŸMac

1. å®‰è£… PostgreSQL 15 å’Œ LibreOfficeï¼š
    ```sh
    brew install postgresql@15 -y
    brew install --cask libreoffice -y
    ```
2. å¯åŠ¨ PostgreSQLï¼š
    ```sh
    brew services start postgresql@15
    ```

#### ğŸ–¥ï¸Windows

1. å®‰è£… PostgreSQL 15 å’Œ LibreOfficeï¼š

    ```sh
    choco install postgresql15 --version=15.0.1 -y
    choco install libreoffice -y
    ```

    _ä½ å¯èƒ½éœ€è¦ä»¥ç®¡ç†å‘˜èº«ä»½è¿è¡Œ cmd_

    > ä¹Ÿå¯ä»¥ä»[è¿™é‡Œ](https://www.libreoffice.org/download/download/)ä¸‹è½½ libreoffice

2. å¯åŠ¨ PostgreSQLï¼š
    ```sh
    pg_ctl -D "C:\Program Files\PostgreSQL\15\data" start
    ```

ç„¶åï¼Œä½ éœ€è¦è¿ç§»æ•°æ®åº“ï¼š

> ä½ éœ€è¦åœ¨ `setup_db.sh` æ–‡ä»¶ä¸­æ›´æ”¹ `DB_ADMIN` å’Œ `DB_PASSWORD`ã€‚

```bash
# ç¡®ä¿ä½ åœ¨ E2M/app ç›®å½•
# è¯·å°† DB_ADMIN å’Œ DB_PASSWORD æ›´æ”¹ä¸ºä½ è‡ªå·±çš„è®¾ç½®
chmod +x ./setup_db.sh


./setup_db.sh
```

ç„¶åï¼Œä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯åŠ¨ APIï¼š

```bash
flask run --host 0.0.0.0 --port=8765 # --debug
```

å¦‚æœä½ æƒ³è¦ä¸€ä¸ªç½‘é¡µï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯åŠ¨ç½‘é¡µï¼š

```bash
cd web
npm install
npm run start
```

### ğŸ”§ è®¾ç½®å¼€å‘ç¯å¢ƒ

```bash
export FLASK_ENV=development
export FLASK_DEBUG=1
```

### ğŸ­ è®¾ç½®ç”Ÿäº§ç¯å¢ƒ

```bash
export FLASK_ENV=production
export FLASK_DEBUG=0
```

### ğŸ“– å¦‚ä½•ä½¿ç”¨

bash è„šæœ¬ï¼š

```bash
curl -X POST "http://127.0.0.1:8765/api/v1/convert" \
  -H "accept: application/json" \
  -H "Content-Type: multipart/form-data; charset=utf-8" \
  -H "Accept-Charset: utf-8" \
  -F "file=@/path/to/file.docx" \
  -F "parse_mode=auto"
```

è¿”å›ï¼š

```json
{
    "message": "è¿™æ˜¯ä½ çš„ markdown å†…å®¹"
}
```

### ğŸ”– è¯­è¨€æ”¯æŒ

ç›®å‰æš‚æ—¶åªæ”¯æŒä¸­æ–‡å’Œè‹±æ–‡ã€‚

```json
{
    "af": "å—éè·å…°è¯­",
    "am": "é˜¿å§†å“ˆæ‹‰è¯­",
    "ar": "é˜¿æ‹‰ä¼¯è¯­",
    "as": "é˜¿è¨å§†è¯­",
    "az": "é˜¿å¡æ‹œç–†è¯­",
    "be": "ç™½ä¿„ç½—æ–¯è¯­",
    "bg": "ä¿åŠ åˆ©äºšè¯­",
    "bn": "å­ŸåŠ æ‹‰è¯­",
    "br": "å¸ƒåˆ—å¡”å°¼è¯­",
    "bs": "æ³¢æ–¯å°¼äºšè¯­",
    "ca": "åŠ æ³°ç½—å°¼äºšè¯­",
    "cs": "æ·å…‹è¯­",
    "cy": "å¨å°”å£«è¯­",
    "da": "ä¸¹éº¦è¯­",
    "de": "å¾·è¯­",
    "el": "å¸Œè…Šè¯­",
    "en": "è‹±è¯­",
    "eo": "ä¸–ç•Œè¯­",
    "es": "è¥¿ç­ç‰™è¯­",
    "et": "çˆ±æ²™å°¼äºšè¯­",
    "eu": "å·´æ–¯å…‹è¯­",
    "fa": "æ³¢æ–¯è¯­",
    "fi": "èŠ¬å…°è¯­",
    "fr": "æ³•è¯­",
    "fy": "è¥¿å¼—é‡Œæ–¯è¯­",
    "ga": "çˆ±å°”å…°è¯­",
    "gd": "è‹æ ¼å…°ç›–å°”è¯­",
    "gl": "åŠ åˆ©è¥¿äºšè¯­",
    "gu": "å¤å‰æ‹‰ç‰¹è¯­",
    "ha": "è±ªè¨è¯­",
    "he": "å¸Œä¼¯æ¥è¯­",
    "hi": "å°åœ°è¯­",
    "hr": "å…‹ç½—åœ°äºšè¯­",
    "hu": "åŒˆç‰™åˆ©è¯­",
    "hy": "äºšç¾å°¼äºšè¯­",
    "id": "å°åº¦å°¼è¥¿äºšè¯­",
    "is": "å†°å²›è¯­",
    "it": "æ„å¤§åˆ©è¯­",
    "ja": "æ—¥è¯­",
    "jv": "çˆªå“‡è¯­",
    "ka": "æ ¼é²å‰äºšè¯­",
    "kk": "å“ˆè¨å…‹è¯­",
    "km": "é«˜æ£‰è¯­",
    "kn": "å¡çº³è¾¾è¯­",
    "ko": "éŸ©è¯­",
    "ku": "åº“å°”å¾·è¯­",
    "ky": "å‰å°”å‰æ–¯è¯­",
    "la": "æ‹‰ä¸è¯­",
    "lo": "è€æŒè¯­",
    "lt": "ç«‹é™¶å®›è¯­",
    "lv": "æ‹‰è„±ç»´äºšè¯­",
    "mg": "é©¬è¾¾åŠ æ–¯åŠ è¯­",
    "mk": "é©¬å…¶é¡¿è¯­",
    "ml": "é©¬æ‹‰é›…æ‹‰å§†è¯­",
    "mn": "è’™å¤è¯­",
    "mr": "é©¬æ‹‰åœ°è¯­",
    "ms": "é©¬æ¥è¯­",
    "my": "ç¼…ç”¸è¯­",
    "ne": "å°¼æ³Šå°”è¯­",
    "nl": "è·å…°è¯­",
    "no": "æŒªå¨è¯­",
    "om": "å¥¥ç½—è«è¯­",
    "or": "å¥¥é‡Œäºšè¯­",
    "pa": "æ—é®æ™®è¯­",
    "pl": "æ³¢å…°è¯­",
    "ps": "æ™®ä»€å›¾è¯­",
    "pt": "è‘¡è„ç‰™è¯­",
    "ro": "ç½—é©¬å°¼äºšè¯­",
    "ru": "ä¿„è¯­",
    "sa": "æ¢µè¯­",
    "sd": "ä¿¡å¾·è¯­",
    "si": "åƒ§ä¼½ç½—è¯­",
    "sk": "æ–¯æ´›ä¼å…‹è¯­",
    "sl": "æ–¯æ´›æ–‡å°¼äºšè¯­",
    "so": "ç´¢é©¬é‡Œè¯­",
    "sq": "é˜¿å°”å·´å°¼äºšè¯­",
    "sr": "å¡å°”ç»´äºšè¯­",
    "su": "å·½ä»–è¯­",
    "sv": "ç‘å…¸è¯­",
    "sw": "æ–¯ç“¦å¸Œé‡Œè¯­",
    "ta": "æ³°ç±³å°”è¯­",
    "te": "æ³°å¢å›ºè¯­",
    "th": "æ³°è¯­",
    "tl": "å¡”åŠ æ´›è¯­",
    "tr": "åœŸè€³å…¶è¯­",
    "ug": "ç»´å¾å°”è¯­",
    "uk": "ä¹Œå…‹å…°è¯­",
    "ur": "ä¹Œå°”éƒ½è¯­",
    "uz": "ä¹Œå…¹åˆ«å…‹è¯­",
    "vi": "è¶Šå—è¯­",
    "xh": "ç§‘è¨è¯­",
    "yi": "æ„ç¬¬ç»ªè¯­",
    "zh": "ä¸­æ–‡"
}
```

## ğŸ¤ å¦‚ä½•è´¡çŒ®

### ğŸŒ¿ åˆ›å»ºæ–°åˆ†æ”¯

åœ¨æäº¤ä»£ç ä¹‹å‰ï¼Œè¯·åˆ›å»ºä¸€ä¸ªæ–°åˆ†æ”¯ï¼š

-   `feature/xxx` ç”¨äºæ–°åŠŸèƒ½
-   `bugfix/xxx` ç”¨äºä¿®å¤é”™è¯¯

ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤åˆ›å»ºä¸€ä¸ªæ–°åˆ†æ”¯ï¼š

```bash
# è·å–æœ€æ–°ä»£ç 
git checkout main
git pull
# åˆ›å»ºæ–°åˆ†æ”¯
git checkout -b feature/xxx
```

### ğŸ“PEP8 é£æ ¼

ç„¶åï¼Œè¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥æ ¼å¼åŒ–ä½ çš„ä»£ç ï¼š

```bash
# æ‰€æœ‰è´¡çŒ®éƒ½åº”éµå¾ª PEP8 é£æ ¼
flake8 .  # æ£€æŸ¥ä»£ç é£æ ¼
black .  # æ ¼å¼åŒ–ä»£ç 
pymarkdownlnt fix .  # æ ¼å¼åŒ– markdown
cd app
poetry export -f requirements.txt --without-hashes > requirements.txt
poetry export -f requirements.txt --without-hashes --with dev -o requirements-dev.txt
```

### ğŸ”„ æ¨é€åˆ°è¿œç¨‹ä»“åº“

```bash
# æ·»åŠ æ›´æ”¹
git add .
# æäº¤æ›´æ”¹
git commit -m "ä½ çš„æäº¤ä¿¡æ¯"
# æ¨é€æ›´æ”¹
git push origin feature/xxx # æˆ–è€…ç®€å•åœ° `git push`
```

### ğŸ³ æ¨é€åˆ° Docker

æ–°ç‰ˆæœ¬ï¼š

```
cd app
docker build -t jingyilin/e2m-api:<version> .
docker push jingyilin/e2m-api:<version>
cd ../web
docker build -t jingyilin/e2m-web:<version> .
docker push jingyilin/e2m-web:<version>
```

ä¾‹å¦‚ï¼Œç‰ˆæœ¬æ˜¯ `v1.0.0`ï¼š

```
cd app
docker build -t jingyilin/e2m-api:v1.0.0 .
docker push jingyilin/e2m-api:v1.0.0
cd ../web
docker build -t jingyilin/e2m-web:v1.0.0 .
docker push jingyilin/e2m-web:v1.0.0
```

### ğŸ”€ æ‹‰å–è¯·æ±‚

```bash
# åœ¨ GitHub ä¸Šåˆ›å»ºä¸€ä¸ªåˆ° develop åˆ†æ”¯çš„æ‹‰å–è¯·æ±‚
```

## ğŸŒŸ è´¡çŒ®è€…

### ğŸ‘¥ è´¡çŒ®è€…åå•

<a href="https://github.com/Jing-yilin/E2M/graphs/contributors">
  <img src="https://contrib.rocks/image?repo=Jing-yilin/E2M" />
</a>

## ğŸ“± ç¤¾åŒº

### å¾®ä¿¡

<img src="assets/wechat_community.jpg" width="30%">

### Discord

<img src="assets/discord_community.png" width="30%">
